# ==============================
# –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô (–≤ –∫–æ–Ω—Å–æ–ª–∏):
#   pip install transformers datasets accelerate torch soundfile pydub nltk
#
# –£–°–¢–ê–ù–û–í–ò–¢–¨ Git LFS:
#   git lfs install
#
# –ö–õ–û–ù–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô:
#   git clone https://huggingface.co/parler-tts/parler_tts_large
#   git clone https://huggingface.co/parler-tts/parler_tts_mini
# ==============================

import os
import torch
import soundfile as sf
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from pydub import AudioSegment
import nltk

# –°–∫–∞—á–∞—Ç—å punkt –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
nltk.download("punkt", quiet=True)

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
INPUT_FILE = "input.txt"        # –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª (–¥–æ 5000 —Å–∏–º–≤–æ–ª–æ–≤)
OUTPUT_DIR = "output_audio"     # –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
FINAL_FILE = "final_output.wav" # –∏–º—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞

USE_LARGE = False   # True = large, False = mini
VOICE = "female"    # "male" –∏–ª–∏ "female"
SPEED = "medium"    # "slow", "medium", "fast"
EMOTION = "neutral" # "neutral", "happy", "sad", "angry", "excited"

# === –£–°–¢–†–û–ô–°–¢–í–û ===
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# === –í–´–ë–û–† –ú–û–î–ï–õ–ò ===
model_path = "./parler_tts_large" if USE_LARGE else "./parler_tts_mini"

# === –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===
processor = AutoProcessor.from_pretrained(model_path)
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_path, torch_dtype=dtype
).to(device)

# === –§–£–ù–ö–¶–ò–Ø: —Å–æ–∑–¥–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ ===
def build_prompt(voice="female", speed="medium", emotion="neutral"):
    parts = []
    if voice == "female":
        parts.append("female voice")
    elif voice == "male":
        parts.append("male voice")

    if speed in ["slow", "medium", "fast"]:
        parts.append(f"{speed} speed")

    if emotion != "neutral":
        parts.append(f"{emotion} emotion")

    return "A " + ", ".join(parts) + ", no background noise"

prompt_voice = build_prompt(VOICE, SPEED, EMOTION)

# === –§–£–ù–ö–¶–ò–Ø: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ ===
def generate_audio(text, filename):
    inputs = processor(text=text, prompt=prompt_voice, return_tensors="pt").to(device)
    with torch.no_grad():
        generated_audio = model.generate(**inputs, max_new_tokens=1000)
    waveform = processor.batch_decode(generated_audio, return_tensors="pt")
    audio = waveform.cpu().numpy().squeeze()
    sf.write(filename, audio, 16000)

# === –°–û–ó–î–ê–ï–ú –ü–ê–ü–ö–£ –î–õ–Ø –í–´–•–û–î–ê ===
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === –ß–¢–ï–ù–ò–ï –í–•–û–î–ù–û–ì–û –¢–ï–ö–°–¢–ê ===
with open(INPUT_FILE, "r", encoding="utf-8") as f:
    text = f.read().strip()

if len(text) > 5000:
    raise ValueError("‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π! –ú–∞–∫—Å–∏–º—É–º 5000 —Å–∏–º–≤–æ–ª–æ–≤.")

# === –†–ê–ó–ë–ò–ï–ù–ò–ï –ù–ê –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø ===
sentences = nltk.sent_tokenize(text)
print(f"üîπ –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(sentences)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤.")

# === –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–õ–Ø –ö–ê–ñ–î–û–ì–û –°–ï–ì–ú–ï–ù–¢–ê ===
segments_audio = []
for i, sentence in enumerate(sentences, 1):
    filename = os.path.join(OUTPUT_DIR, f"segment_{i}.wav")
    print(f"üéô –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ {i}/{len(sentences)}: {sentence[:60]}...")
    generate_audio(sentence, filename)
    segments_audio.append(AudioSegment.from_wav(filename))

# === –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í –û–î–ò–ù –§–ê–ô–õ ===
final_audio = AudioSegment.silent(duration=0)
for seg in segments_audio:
    final_audio += seg + AudioSegment.silent(duration=300)  # –ø–∞—É–∑–∞ 0.3—Å

final_path = os.path.join(OUTPUT_DIR, FINAL_FILE)
final_audio.export(final_path, format="wav")

print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {final_path}")