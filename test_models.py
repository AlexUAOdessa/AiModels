import os
import torch
from diffusers import AutoPipelineForText2Image, KandinskyV22PriorPipeline
from PIL import Image
from huggingface_hub import HfApi  # –î–æ–±–∞–≤–∏–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏/—Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º

# –î–ª—è —Ä–∞–±–æ—Ç—ã —ç—Ç–æ–≥–æ –∫–æ–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install diffusers transformers accelerate torch pillow huggingface_hub
# 2. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–≤–µ –º–æ–¥–µ–ª–∏ (–∏–ª–∏ —Å–∫–∞—á–∞—Ç—å —á–µ—Ä–µ–∑ CLI):
#    - Prior: git clone https://huggingface.co/kandinsky-community/kandinsky-2-2-prior
#    - Decoder: git clone https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder
# 3. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ Git LFS —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: git lfs install
# 4. –ï—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç—ã ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ VPN –∏–ª–∏ huggingface-cli download

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ (—É–∫–∞–∂–∏—Ç–µ —Å–≤–æ–∏)
model_paths = {
    "kandinsky-community/kandinsky-2-2-decoder": "./kandinsky-2-2-decoder",
    "kandinsky-community/kandinsky-2-2-prior": "./kandinsky-2-2-prior",
}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
def check_model_complete(local_path, model_id):
    required_files = ['config.json', 'model.safetensors.index.json']  # –ë–∞–∑–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    if not os.path.exists(local_path):
        return False
    for file in required_files:
        if not os.path.exists(os.path.join(local_path, file)):
            print(f"‚ö†Ô∏è Missing {file} in {local_path}. Model incomplete ‚Äî will try to download.")
            return False
    print(f"‚úÖ Model {local_path} seems complete.")
    return True

# –ü–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
prompt = "A spaceship is flying toward a black hole"
negative_prompt = "low resolution, blurry, bad anatomy"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª–∏ –∑–∞—Ä–∞–Ω–µ–µ
for model_name, local_path in model_paths.items():
    check_model_complete(local_path, model_name)

for model_name, local_path in model_paths.items():
    print(f"\n=== Loading {model_name} from {local_path} ===")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –ø–æ–ª–Ω–∞—è
    if not os.path.exists(local_path) or not check_model_complete(local_path, model_name):
        print(f"‚ùå Local path '{local_path}' incomplete. Trying fallback download...")
        try:
            # Fallback: –°–∫–∞—á–∏–≤–∞–µ–º —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º (—á–µ—Ä–µ–∑ HfApi)
            api = HfApi()
            api.hf_hub_download(
                repo_id=model_name,
                filename="model.safetensors",  # –ü—Ä–∏–º–µ—Ä; diffusers —Å–∞–º —Å–∫–∞—á–∞–µ—Ç –≤—Å–µ
                local_dir=local_path,
                timeout=300  # 5 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
            )
            print(f"‚úÖ Fallback download attempted for {model_name}")
        except Exception as e:
            print(f"‚ùå Fallback failed: {e}. Use VPN or CLI download.")
            continue

    # –ó–∞–≥—Ä—É–∂–∞–µ–º prior (–µ—Å–ª–∏ —ç—Ç–æ prior-–º–æ–¥–µ–ª—å)
    if "prior" in model_name:
        pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
            local_path,
            torch_dtype=torch.float16,
            local_files_only=True  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã, –±–µ–∑ –æ–Ω–ª–∞–π–Ω
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe_prior = pipe_prior.to(device)
        print(f"‚úÖ Prior pipeline loaded on {device}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
        out = pipe_prior(
            prompt,
            negative_prompt=negative_prompt,
            height=512,
            width=512,
            num_inference_steps=25,
            guidance_scale=4.0
        )
        image_embeds = out.image_embeds
        negative_image_embeds = out.negative_image_embeds
        print(f"‚úÖ Embeddings generated: shape {image_embeds.shape}")
        continue

    # –î–ª—è decoder
    elif "decoder" in model_name:
        pipe = AutoPipelineForText2Image.from_pretrained(
            local_path,
            torch_dtype=torch.float16,
            local_files_only=True  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe = pipe.to(device)
        print(f"‚úÖ Decoder pipeline loaded on {device}")

        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = pipe(
                image_embeds=image_embeds,
                negative_image_embeds=negative_image_embeds,
                height=512,
                width=512,
                guidance_scale=7.5,
                num_inference_steps=50
            ).images[0]
            print(f"‚úÖ Image generated successfully")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            fname = model_name.replace("/", "_") + "_out.png"
            image.save(fname)
            print(f"‚úÖ Saved image to {fname}")
        except NameError:
            print("‚ùå Error: Run prior first to get embeddings!")
        except Exception as e:
            print(f"‚ùå Generation error: {e}")

print("\nüéâ Process complete! Check the saved PNG file.")