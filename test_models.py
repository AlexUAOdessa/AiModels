import os
import torch
from diffusers import AutoPipelineForText2Image, KandinskyV22PriorPipeline
from PIL import Image

# –î–ª—è —Ä–∞–±–æ—Ç—ã —ç—Ç–æ–≥–æ –∫–æ–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:  pip install diffusers transformers accelerate torch pillow
# 2. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–≤–µ –º–æ–¥–µ–ª–∏:
#    - Prior: git clone https://huggingface.co/kandinsky-community/kandinsky-2-2-prior
#    - Decoder: git clone https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder
# 3. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ Git LFS —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: git lfs install

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ (—É–∫–∞–∂–∏—Ç–µ —Å–≤–æ–∏)
model_paths = {
    "kandinsky-community/kandinsky-2-2-decoder": "./kandinsky-2-2-decoder",
    "kandinsky-community/kandinsky-2-2-prior": "./kandinsky-2-2-prior",
}

# –ü–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
prompt = "A spaceship is flying toward a black hole"
negative_prompt = "low resolution, blurry, bad anatomy"

for model_name, local_path in model_paths.items():
    print(f"\n=== Loading {model_name} from {local_path} ===")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not os.path.exists(local_path):
        print(f"‚ùå Local path '{local_path}' not found. Please download the model manually.")
        continue

    # –ó–∞–≥—Ä—É–∂–∞–µ–º prior (–µ—Å–ª–∏ —ç—Ç–æ prior-–º–æ–¥–µ–ª—å)
    if "prior" in model_name:
        pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
            local_path,
            torch_dtype=torch.float16
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe_prior = pipe_prior.to(device)
        print(f"‚úÖ Prior pipeline loaded on {device}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
        out = pipe_prior(
            prompt,
            negative_prompt=negative_prompt,
            height=512,
            width=512,
            num_inference_steps=25,
            guidance_scale=4.0
        )
        image_embeds = out.image_embeds
        negative_image_embeds = out.negative_image_embeds
        print(f"‚úÖ Embeddings generated: shape {image_embeds.shape}")
        continue

    # –î–ª—è decoder
    elif "decoder" in model_name:
        pipe = AutoPipelineForText2Image.from_pretrained(
            local_path,
            torch_dtype=torch.float16
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe = pipe.to(device)
        print(f"‚úÖ Decoder pipeline loaded on {device}")

        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = pipe(
                image_embeds=image_embeds,
                negative_image_embeds=negative_image_embeds,
                height=512,
                width=512,
                guidance_scale=7.5,
                num_inference_steps=50
            ).images[0]
            print(f"‚úÖ Image generated successfully")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            fname = model_name.replace("/", "_") + "_out.png"
            image.save(fname)
            print(f"‚úÖ Saved image to {fname}")
        except NameError:
            print("‚ùå Error: Run prior first to get embeddings!")
        except Exception as e:
            print(f"‚ùå Generation error: {e}")

print("\nüéâ Process complete! Check the saved PNG file.")